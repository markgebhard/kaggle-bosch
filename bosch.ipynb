{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle - Bosch Production Line Performance \n",
    "### (Handling Large Data With Limited Memory)\n",
    "\n",
    "Welcome! This jupyter notebook will demonstrate how to work with large datasets in python by analyzing production line data associated with the Bosch Kaggle competition (https://www.kaggle.com/c/bosch-production-line-performance). \n",
    "\n",
    "[This notebook is still a work in progress and will be updated as I improve algorithm performance.]\n",
    "\n",
    "Questions, comments, suggestions, and corrections can be sent to mgebhard@gmail.com.\n",
    "\n",
    "## Business Challenge\n",
    "Bosch, a manufacturing company, teamed up with Kaggle to challenge teams to create a classification algorithm that predicts \"internal failures along the manufacturing process using thousands of measurements and tests made for each component along the assembly line.\"\n",
    "\n",
    "\n",
    "## Data\n",
    "Bosch provided six huge files worth of data for the challenge (https://www.kaggle.com/c/bosch-production-line-performance/data). Three sets of training data--numeric, categorical, and dates--and the equivalent sets of test data. They contain a large number of features (one of the largest sets ever hosted on Kaggle), and the uncompressed files come out to **14.3 GB**. \n",
    "\n",
    "One of the largest difficulties associated with the competition is handing this amount of data. One strategy is to move the data to Amazon Web Services and use big data tools like Spark and Hadoop. Often, however, we are forced to extract value from data given real-world constraints like less memory and processing power. In this notebook, I'll work through an alternative approach where I split and simplify the data in order to process it on my 8GB RAM laptop.\n",
    "\n",
    "Let's start by examining the training data. Because the files are so large, we can't do the usual practice of using pandas to read the .CSV file into a dataframe. Instead, let's just look at a few lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id,L0_S0_F0,L0_S0_F2,L0_S0_F4,L0_S0_F6,L ... 4258,L3_S51_F4260,L3_S51_F4262,Response\n",
      "\n",
      "4,0.03,-0.034,-0.197,-0.179,0.118,0.116, ... ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0\n",
      "\n",
      "6,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, ... ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0\n",
      "\n",
      "7,0.088,0.086,0.003,-0.052,0.161,0.025,- ... ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0\n",
      "\n",
      "9,-0.036,-0.064,0.294,0.33,0.074,0.161,0 ... ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0\n",
      "\n",
      "11,-0.055,-0.086,0.294,0.33,0.118,0.025, ... ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "line_count = 0\n",
    "extracted_lines = []\n",
    "with open('train_numeric.csv') as f:\n",
    "    for line in f:\n",
    "        if line_count < 6:\n",
    "            extracted_lines.append(line)\n",
    "            line_count += 1\n",
    "        else:\n",
    "            break\n",
    "for line in extracted_lines:\n",
    "    print line[:40], '...', line[-40:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that each line in train_numeric.csv represents a component with an Id, a long list of features (many of which are blank), and a Response indicating passage or failure of QC. Further examination shows that only 0.58% of Responses are failures, or *1*.\n",
    "\n",
    "Because we already have more data than we can handle, we're going to simplify by only working with train_numeric.csv and disregard train_categorical.csv and train_date.csv. Furthermore, we need to deal with the fact that train_numeric.csv is larger than we can handle and also is highly imbalanced. To do this, we're going to pull out all of the rows with Positive responses and randomly sample an equivalent number of negative rows. We'll make a new .CSV file that is 1/100th the size of the original and is now equally balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "line_count = 0\n",
    "extracted_positive_lines = []\n",
    "with open('train_numeric.csv') as f:\n",
    "    for line in f:\n",
    "        if line_count == 0:\n",
    "            extracted_positive_lines.append(line)\n",
    "            line_count += 1\n",
    "        elif line[-2] == '1':\n",
    "            extracted_positive_lines.append(line)\n",
    "\n",
    "line_count = 0\n",
    "extracted_negative_lines = []\n",
    "with open('train_numeric.csv') as f:\n",
    "    for line in f:\n",
    "        if line_count == 0:\n",
    "            line_count += 1\n",
    "            continue\n",
    "        if line_count > 0 and random.random() < 0.0058:\n",
    "            extracted_negative_lines.append(line)\n",
    "\n",
    "combined_extracted_lines = extracted_positive_lines + extracted_negative_lines\n",
    "with open('train_numeric_short.csv', 'w') as f:\n",
    "    for line in combined_extracted_lines:\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can move the new .CSV to a pandas dataframe and replace the empty features with *0*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13726, 970)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_numeric_short_df = pd.read_csv('train_numeric_short.csv')\n",
    "train_numeric_short_df.fillna(value=0, inplace=True)\n",
    "train_numeric_short_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now working with 13769 samples with 968 features not including Id and Response. Let's use train_test_split from sklearn.cross_validation to split our training data, which will let us quickly evaluate and compare various classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X = train_numeric_short_df.drop(['Response', 'Id'], axis=1)\n",
    "y = train_numeric_short_df['Response']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Classifiers\n",
    "With our training data split into new training and test sets, we can feed it into various sci-kit learn classifiers. The Kaggle competition is being judged using the Matthews correlation coefficient, so we'll use that to find the best classifier.\n",
    "* https://en.wikipedia.org/wiki/Matthews_correlation_coefficient\n",
    "* http://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html\n",
    "\n",
    "Additionally, we can use [recursive feature elimination with cross-validation](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html). Our data set is high dimensional with 968 features. Removing features of low importance can reduce the model complexity, overfitting, and training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.feature_selection import RFECV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start with a simple [logistic regression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) combined with the recursive feature elimination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1774656753667388"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = RFECV(LogisticRegression(), step=200)\n",
    "clf.fit(X_train, y_train)\n",
    "y_output = clf.predict(X_test)\n",
    "matthews_corrcoef(y_test, y_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's try a [linear SVC model](http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1655979020934964"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "clf = RFECV(LinearSVC(), step=200)\n",
    "clf.fit(X_train, y_train)\n",
    "y_output = clf.predict(X_test)\n",
    "matthews_corrcoef(y_test, y_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the [ExtraTreesClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29972361676295534"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "forest = ExtraTreesClassifier(n_estimators=250, random_state=0)\n",
    "clf = RFECV(forest, step=200)\n",
    "clf.fit(X_train, y_train)\n",
    "y_output = clf.predict(X_test)\n",
    "matthews_corrcoef(y_test, y_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've settled on the ExtraTreesClassifier, let's retrain it using our full training set from before we split it with train_test_split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RFECV(cv=None,\n",
       "   estimator=ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=250, n_jobs=1,\n",
       "           oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "   estimator_params=None, scoring=None, step=200, verbose=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're ready to analyze the actual test data provided by Bosch. As with the training data, though, the 2.1 GB file is quite large for my laptop. We can split up the test data into files of 100000 lines each and get predictions for each smaller file and then stitch the predictions back together for a final submission file.\n",
    "\n",
    "Fortunately, pandas can read .CSV files in chunks which makes it easy to split up the test data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('test_numeric.csv', chunksize=100000)\n",
    "file_number = 0\n",
    "for chunk in test:\n",
    "    path = 'test_data/short' + str(file_number) + '.csv'\n",
    "    chunk.to_csv(path)\n",
    "    file_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(12):\n",
    "    test_numeric_short_df = pd.read_csv('test_data/short' + str(i) + '.csv').fillna(value=0)\n",
    "    Ids = test_numeric_short_df.ix[:,'Id']\n",
    "    X_test_real = test_numeric_short_df.drop(['Id', 'Unnamed: 0'], axis=1)\n",
    "    y_output_real = selector.predict(X_test_real)\n",
    "    output = pd.Series(y_output_real, name='Response')\n",
    "    output = pd.concat([Ids, output], axis=1)\n",
    "    output.to_csv('test_output/test_output' + str(i) + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just have to put our prediction files together into a single file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.copyfile('test_output/test_output0.csv', 'test_output/output_combined.csv')\n",
    "\n",
    "output_combined = open('test_output/output_combined.csv', 'a')\n",
    "for i in range(1,12):\n",
    "    lines = open('test_output/test_output' + str(i) + '.csv', 'r').readlines()\n",
    "    for line in lines[1:]:\n",
    "        output_combined.write(line)\n",
    "output_combined.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Conclusions\n",
    "\n",
    "Submitting our file to Kaggle gets us a score of 0.04623. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
